{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UogJSKKQRg5g"
      },
      "source": [
        "## **1. Install and import the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRdhReXrYIsN"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 selenium requests langchain chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp5E3G2PDZUt"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY6_P3ZeeZ3q"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyiiV3-BEOBD"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHlPfCfnC11n"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53DidMRQbrBB"
      },
      "outputs": [],
      "source": [
        "!pip install gradio pyttsx3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFJ1_2GnezHk"
      },
      "source": [
        "# **Chatbot - updated version**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZlmU_gsDHyh",
        "outputId": "c03a59dc-de22-4754-82a1-72b6b4c5090a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Total extracted documents: 9\n",
            "üìë Split into 237 document chunks.\n",
            "‚úÖ Successfully stored 533524 document chunks in Chroma!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF for extracting text from PDFs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import openai\n",
        "import chromadb\n",
        "chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
        "\n",
        "# üîπ Path to the text file and PDF files\n",
        "text_file_path = \"/content/drive/MyDrive/Analytics_Practicum_project/combined_final.json\" # Change directory to location where the JSON files are stored.\n",
        "pdf_directory = \"/content/drive/MyDrive/Analytics_Practicum_project/pdfs/\"  # Change directory to location where the PDF files are stored.\n",
        "documents = []\n",
        "\n",
        "# üîπ Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in document:\n",
        "        text += page.get_text()  # Extract text from each page\n",
        "    return text\n",
        "\n",
        "# üîπ Read text file content\n",
        "with open(text_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# üîπ Add the text content from the file\n",
        "entries = content.split(\"====================================================================================================\")\n",
        "for entry in entries:\n",
        "    entry = entry.strip()\n",
        "    if not entry:\n",
        "        continue\n",
        "\n",
        "    # üîπ Extract TITLE and CONTENT\n",
        "    title_line = \"\"\n",
        "    content_text = \"\"\n",
        "\n",
        "    for line in entry.splitlines():\n",
        "        if line.startswith(\"TITLE:\"):\n",
        "            title_line = line.replace(\"TITLE:\", \"\").strip()\n",
        "        elif line.startswith(\"CONTENT:\"):\n",
        "            content_text = entry.split(\"CONTENT:\")[1].strip()\n",
        "            break\n",
        "\n",
        "    full_text = f\"{title_line}\\n\\n{content_text}\"\n",
        "    if full_text.strip():\n",
        "        documents.append(full_text)\n",
        "\n",
        "# üîπ Add the content from PDF files\n",
        "for filename in os.listdir(pdf_directory):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_directory, filename)\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "        documents.append(pdf_text)\n",
        "\n",
        "print(f\"üìÑ Total extracted documents: {len(documents)}\")\n",
        "\n",
        "# üîπ Proceed with the same text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.create_documents(documents)\n",
        "\n",
        "print(f\"üìë Split into {len(chunks)} document chunks.\")\n",
        "\n",
        "# üîπ Set up OpenAI API key and embeddings\n",
        "openai.api_key = \"<your OPENAI API key>\"\n",
        "embedding_function = OpenAIEmbeddings(openai_api_key=\"<your OPENAI API key>\")\n",
        "\n",
        "# üîπ Store document embeddings in ChromaDB\n",
        "persist_directory = \"/content/drive/MyDrive/Analytics_Practicum_project/vectorstore\" # Change directory location as needed.\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding_function, persist_directory=persist_directory)\n",
        "\n",
        "print(f\"‚úÖ Successfully stored {len(vectorstore.get()['documents'])} document chunks in Chroma!\")\n",
        "\n",
        "# üîπ Define Retriever for finding relevant documents\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "# üîπ Define RAG chain (no changes needed)\n",
        "from langchain import hub\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are an expert assistant. Use the following retrieved context to generate a complete, well-structured, and contextually rich answer.\n",
        "Provide definitions, relevant details, and connect ideas clearly, even if the original context is concise. If i ask you any question that is not related to the Jindal School of Management (JSOM), just answer as I don't know, provide more context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer in full detail:\n",
        "\"\"\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "\n",
        "# üîπ Load a standard RAG prompt\n",
        "prompt = custom_prompt\n",
        "\n",
        "# üîπ Define Language Model (GPT-3.5 Turbo)\n",
        "llm = ChatOpenAI(openai_api_key=\"<your OPENAI API key>\", model_name=\"gpt-3.5-turbo\", temperature=0) # Insert your OPENAI API key\n",
        "\n",
        "# üîπ Function to format retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs if doc.page_content.strip())\n",
        "\n",
        "# üîπ Define RAG Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# üîπ Initialize conversation history\n",
        "conversation_history = []\n",
        "\n",
        "# üîπ Function to generate chatbot responses\n",
        "def generate_response(question):\n",
        "    global conversation_history, last_question, last_response\n",
        "    last_question = question\n",
        "\n",
        "    # Ensure question is valid\n",
        "    if not question or not isinstance(question, str):\n",
        "        return \"Invalid input. Please ask a relevant question.\"\n",
        "\n",
        "    question_lower = question.lower().strip()\n",
        "\n",
        "    # Predefined quick responses\n",
        "    greetings = {\"hi\", \"hello\", \"hey\", \"hola\"}\n",
        "    thanks = {\"Thank you!\", \"thank you\", \"Thanks!\", \"thanks\", \"thx\", \"ty!\"}\n",
        "\n",
        "    if question_lower in greetings:\n",
        "        return \"Hello! How can I assist you today?\"\n",
        "    elif question_lower in thanks:\n",
        "        return \"You're welcome! Let me know if you need more help.\"\n",
        "\n",
        "    # Retrieve context from Chroma\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = format_docs(retrieved_docs)\n",
        "\n",
        "    # If no context is found, return a relevant response\n",
        "    if not formatted_context.strip():\n",
        "        return \"I couldn't find relevant information in my database. Can you clarify or provide more details?\"\n",
        "\n",
        "    # Prepare the input for RAG model\n",
        "    full_input = f\"Context:\\n{formatted_context}\\n\\nQuestion: {question}\"\n",
        "\n",
        "    # Generate response using the RAG chain\n",
        "    response = rag_chain.invoke(full_input)\n",
        "\n",
        "    # Append conversation history\n",
        "    conversation_history.append(f\"User: {question}\\nBot: {response}\")\n",
        "    response = rag_chain.invoke(full_input)\n",
        "    last_response = response  # save response\n",
        "\n",
        "    conversation_history.append(f\"User: {question}\\nBot: {response}\")\n",
        "    return response\n",
        "\n",
        "def reevaluate_response():\n",
        "    global last_question\n",
        "    if not last_question:\n",
        "        return \"‚ö†Ô∏è No previous question to re-evaluate.\"\n",
        "\n",
        "    # Optionally: modify the prompt or log this for feedback training\n",
        "    improved_response = generate_response(last_question)\n",
        "    return f\"üîÅ Improved Response:\\n{improved_response}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLxwhzruDGv2"
      },
      "source": [
        "## **User Interface**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "1aZUCKksNj-x",
        "outputId": "16b330a0-4b60-4e02-f3e1-cf4779a6e4e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9a073954bcac7f3f07.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://9a073954bcac7f3f07.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9a073954bcac7f3f07.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "# üîπ Load the UTD logo or any relevant image\n",
        "image_path = \"/content/drive/MyDrive/Analytics_Practicum_project/my_robot.png\" # Insert the chatbot UI image\n",
        "robot_image = Image.open(image_path).resize((600, 250))\n",
        "\n",
        "last_question = \"\"\n",
        "last_response = \"\"\n",
        "\n",
        "# üîπ Define Gradio Chatbot UI\n",
        "with gr.Blocks() as iface:\n",
        "    # üîπ Inject custom CSS to center align\n",
        "    gr.HTML(\"\"\"\n",
        "    <style>\n",
        "        .center-text {\n",
        "            text-align: center;\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            align-items: center;\n",
        "        }\n",
        "    </style>\n",
        "    \"\"\")\n",
        "\n",
        "    # üîπ Center the title using gr.Markdown with custom class\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"### <p class='center-text'>üéì CometVerse-One stop solution for JSOM students!\")\n",
        "\n",
        "    # üîπ Center the description using gr.Markdown with custom class\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"<p class='center-text'><b>ü§ñ Get information on courses, admissions, campus life, and more!</b></p>\")\n",
        "\n",
        "    # üîπ Display the image with custom size and center it\n",
        "    with gr.Row():\n",
        "        gr.Image(value=robot_image, label=\"UTD Chatbot\", show_label=False)\n",
        "\n",
        "    # üîπ Define input box and buttons\n",
        "    with gr.Column(scale=1, min_width=300):\n",
        "        input_box = gr.Textbox(label=\"Ask your question\", placeholder=\"Type here...\")\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "        flag_btn = gr.Button(\"Flag Response\", variant=\"stop\")\n",
        "\n",
        "    output_box = gr.Textbox(label=\"Response\", interactive=False)\n",
        "\n",
        "    submit_btn.click(fn=generate_response, inputs=input_box, outputs=output_box)\n",
        "    clear_btn.click(lambda: (\"\", \"\"), inputs=[], outputs=[input_box, output_box])\n",
        "    flag_btn.click(fn=reevaluate_response, inputs=[], outputs=output_box)\n",
        "\n",
        "# üîπ Launch the interface\n",
        "iface.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}