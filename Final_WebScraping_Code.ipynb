{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYqXi-Yqgq84",
        "outputId": "e7776c4f-a05a-45a3-d830-26a4edcc0664",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.30.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fdtwWeouhHMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping all the links from the site-index and storing it as a JSON file\n",
        "import json\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "def scrape_utdallas_site_links(url=\"https://www.utdallas.edu/siteindex/\", output_file=\"scraped_utd_links.json\"):\n",
        "    \"\"\"Scrapes all site links from the specified UTDallas site index page.\"\"\"\n",
        "    try:\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        driver.get(url)\n",
        "\n",
        "        # Scroll to the bottom of the page\n",
        "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        while True:\n",
        "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            time.sleep(2)  # Wait for content to load\n",
        "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                break\n",
        "            last_height = new_height\n",
        "\n",
        "        # Wait for all links to be present.\n",
        "        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'a')))\n",
        "\n",
        "        # Extract links using JavaScript\n",
        "        links = driver.execute_script(\"\"\"\n",
        "            let linkElements = document.querySelectorAll('a');\n",
        "            let links = [];\n",
        "            for (let link of linkElements) {\n",
        "                links.push({\n",
        "                    text: link.textContent,\n",
        "                    href: link.href\n",
        "                });\n",
        "            }\n",
        "            return links;\n",
        "        \"\"\")\n",
        "\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(links, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"Successfully scraped {len(links)} links and saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    finally:\n",
        "        if 'driver' in locals():\n",
        "            driver.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_utdallas_site_links()"
      ],
      "metadata": {
        "id": "_aGZZqcThNEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a80e38-bbb8-4d02-d6c5-cea3d588bc5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully scraped 1181 links and saved to scraped_utd_links.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data scraping - UTD\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "\n",
        "def scrape_and_store(input_json=\"scraped_utd_links.json\", output_folder=\"scraped_sites_ind\", combined_output=\"combined_scraped_data_utd.json\"):\n",
        "    \"\"\"\n",
        "    Reads links from a JSON, scrapes data from each link, stores as individual JSONs, and compiles into a single JSON.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            links_data = json.load(f)\n",
        "        links = [item[\"href\"] for item in links_data]\n",
        "        if not os.path.exists(output_folder):\n",
        "            os.makedirs(output_folder)\n",
        "        scraped_data = {}\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "        for i, link in enumerate(links):\n",
        "            try:\n",
        "                time.sleep(random.uniform(1, 3)) #delay to prevent rate limiting.\n",
        "                response = requests.get(link, headers=headers)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "                text_content = soup.get_text(separator=\"\\n\", strip=True)\n",
        "                filename = os.path.join(output_folder, f\"site_{i}.json\")\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump({\"url\": link, \"content\": text_content}, f, indent=4, ensure_ascii=False)\n",
        "                scraped_data[link] = {\"url\": link, \"content\": text_content, \"children\": {}}\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error accessing {link}: {e}\")\n",
        "                scraped_data[link] = {\"url\": link, \"content\": f\"Error accessing {link}: {e}\", \"children\": {}}\n",
        "        with open(combined_output, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(scraped_data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"Scraped and stored data in {output_folder} and {combined_output}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {input_json} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "scrape_and_store()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmATnuvGzCnz",
        "outputId": "789c4d67-7941-4999-f0ea-91d3f4a27b28",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error accessing https://outlook.office365.com/owa/?realm=utdallas.edu: ('Connection aborted.', HTTPException('got more than 100 headers'))\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing https://urec.utdallas.edu//facilities/index.html#ac: 404 Client Error: Not Found for url: https://urec.utdallas.edu//facilities/index.html#ac\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error accessing https://www.collin.edu/chec/: 404 Client Error: Not Found for url: https://www.collin.edu/chec/\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing https://outlook.office365.com/owa/?realm=utdallas.edu: ('Connection aborted.', HTTPException('got more than 100 headers'))\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing https://news.utdallas.edu/campus-community/inclement-weather-policy-2022/: HTTPSConnectionPool(host='news-2024.utdallas.edu', port=443): Max retries exceeded with url: /campus-community/inclement-weather-closing-policies-2023/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdab894d2d0>: Failed to resolve 'news-2024.utdallas.edu' ([Errno -2] Name or service not known)\"))\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing https://www.gis.utdallas.edu/research/lithocenter/: 404 Client Error: Not Found for url: https://www.utdallas.edu/research/lithocenter/\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing http://www.thea.nesinc.com/: 403 Client Error: Forbidden for url: http://www.thea.nesinc.com/\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "Error accessing tel:972-883-2111: No connection adapters were found for 'tel:972-883-2111'\n",
            "Error accessing mailto:webdeveloper@utdallas.edu: No connection adapters were found for 'mailto:webdeveloper@utdallas.edu'\n",
            "Error accessing https://www.facebook.com/utdallas: 400 Client Error: Bad Request for url: https://www.facebook.com/login/?next=https%3A%2F%2Fwww.facebook.com%2Futdallas\n",
            "Error accessing https://www.instagram.com/ut_dallas/: 429 Client Error: Too Many Requests for url: https://www.instagram.com/accounts/login/?next=https%3A%2F%2Fwww.instagram.com%2Fut_dallas%2F&is_from_rle\n",
            "Scraped and stored data in scraped_sites_ind and combined_scraped_data_utd.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to determine the number of unique utd links\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "def count_json_files(folder_path=\"scraped_sites_ind\"):\n",
        "    \"\"\"\n",
        "    Counts the number of JSON files in the specified folder.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"Folder '{folder_path}' does not exist.\")\n",
        "            return 0\n",
        "\n",
        "        file_count = 0\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(\".json\") and os.path.isfile(os.path.join(folder_path, filename)):\n",
        "                file_count += 1\n",
        "\n",
        "        print(f\"Number of JSON files in '{folder_path}': {file_count}\")\n",
        "        return file_count\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return 0\n",
        "\n",
        "# Example usage in Google Colab:\n",
        "count_json_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa2b4t9o_U8s",
        "outputId": "262ef7b9-1d51-443a-a5c8-214d7bc0e4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of JSON files in 'scraped_sites_ind': 1144\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1144"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to save all the necessary links as a json file.\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Define the input Excel file name and output JSON file name\n",
        "excel_file = \"Links_Necessary.xlsx\"\n",
        "json_file = \"Links_Necessary.json\"\n",
        "\n",
        "try:\n",
        "    # Read the Excel file into a pandas DataFrame\n",
        "    df = pd.read_excel(excel_file)\n",
        "\n",
        "    # Assuming the links are in the first column (index 0).\n",
        "    # Adjust the column index if your links are in a different column.\n",
        "    links_column = df.iloc[:, 0].astype(str)\n",
        "\n",
        "    # Remove serial numbers and extract only the links\n",
        "    extracted_links = []\n",
        "    for item in links_column:\n",
        "        # Split the string by the first space to separate the serial number (if present)\n",
        "        parts = item.split(' ', 1)\n",
        "        if len(parts) > 1:\n",
        "            link = parts[1].strip()\n",
        "            extracted_links.append(link)\n",
        "        else:\n",
        "            # If no space is found, assume the whole item is the link\n",
        "            extracted_links.append(item.strip())\n",
        "\n",
        "    # Remove duplicate links by converting to a set and back to a list\n",
        "    unique_links = sorted(list(set(extracted_links)))\n",
        "\n",
        "    # Get the count of unique links\n",
        "    link_count = len(unique_links)\n",
        "\n",
        "    # Save the unique links to a JSON file\n",
        "    data = {\"links\": unique_links}\n",
        "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Successfully extracted {link_count} unique links from '{excel_file}' and saved them to '{json_file}'.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{excel_file}' was not found. Please make sure the file is in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(f\"Error details: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtal4sShvC3L",
        "outputId": "5626a689-9106-4367-8faf-513345a0d8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted 223 unique links from 'Links_Necessary.xlsx' and saved them to 'Links_Necessary.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data scraping - Links_Necessary\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "def scrape_and_store_combined(input_json=\"Links_Necessary.json\", combined_output=\"combined_scraped_data_links_necessary.json\"):\n",
        "    \"\"\"\n",
        "    Reads links from a JSON file with the structure {\"links\": [...]},\n",
        "    scrapes data from each link, and identifies/scrapes sub-links based on\n",
        "    whether they are within the same domain and path structure.\n",
        "    All scraped data is compiled into a single nested JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "            initial_links = data.get(\"links\", [])\n",
        "            if not initial_links:\n",
        "                print(f\"Warning: No links found in '{input_json}'.\")\n",
        "                return\n",
        "        scraped_data = {}\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        def should_be_sublink(main_url, potential_sub_url):\n",
        "            \"\"\"\n",
        "            Determines if a potential URL should be considered a sub-link\n",
        "            of the main URL based on domain and path.\n",
        "            \"\"\"\n",
        "            main_parsed = urlparse(main_url)\n",
        "            sub_parsed = urlparse(potential_sub_url)\n",
        "\n",
        "            if main_parsed.netloc == sub_parsed.netloc and potential_sub_url.startswith(main_url):\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        def scrape_page(url, parent_data, scraped):\n",
        "            \"\"\"\n",
        "            Scrapes the content of the given URL and recursively scrapes its sub-links.\n",
        "            \"\"\"\n",
        "            if url in scraped:\n",
        "                return  # Avoid re-scraping\n",
        "\n",
        "            try:\n",
        "                time.sleep(random.uniform(1, 3))  # Delay to prevent rate limiting.\n",
        "                response = requests.get(url, headers=headers)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "                text_content = soup.get_text(separator=\"\\n\", strip=True)\n",
        "                parent_data[\"content\"] = text_content\n",
        "                parent_data[\"children\"] = {}\n",
        "                scraped.add(url)\n",
        "\n",
        "                # Find all links on the page\n",
        "                all_links = [urljoin(url, a.get(\"href\")) for a in soup.find_all(\"a\", href=True)]\n",
        "                unique_links_on_page = list(set(all_links))\n",
        "\n",
        "                sub_link_counter = 0\n",
        "                for potential_sub_link in unique_links_on_page:\n",
        "                    if should_be_sublink(url, potential_sub_link) and potential_sub_link != url:\n",
        "                        child_data = {\"url\": potential_sub_link, \"content\": \"\", \"children\": {}}\n",
        "                        parent_data[\"children\"][f\"sub_link_{sub_link_counter}\"] = child_data\n",
        "                        scrape_page(potential_sub_link, child_data, scraped)\n",
        "                        sub_link_counter += 1\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                parent_data[\"content\"] = f\"Error accessing {url}: {e}\"\n",
        "                parent_data[\"children\"] = {}\n",
        "                print(f\"Error accessing {url}: {e}\")\n",
        "            except Exception as e:\n",
        "                parent_data[\"content\"] = f\"Error processing {url}: {e}\"\n",
        "                parent_data[\"children\"] = {}\n",
        "                print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "        scraped_urls = set()\n",
        "        for i, main_link in enumerate(initial_links):\n",
        "            if isinstance(main_link, str) and main_link.startswith(\"http\"):\n",
        "                main_link_data = {\"url\": main_link, \"content\": \"\", \"children\": {}}\n",
        "                scraped_data[f\"main_link_{i}\"] = main_link_data\n",
        "                scrape_page(main_link, main_link_data, scraped_urls)\n",
        "            else:\n",
        "                print(f\"Warning: Skipping invalid link in input: {main_link}\")\n",
        "\n",
        "        with open(combined_output, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(scraped_data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"Scraped and stored data in {combined_output}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {input_json} not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from '{input_json}'. Please ensure the file is valid JSON.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "scrape_and_store_combined()"
      ],
      "metadata": {
        "id": "W98HaFpgytm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}